---
title: "Results and Operationalization"
author: "Michael Messmer"
output:
  html_document:
    df_print: paged
---
<a href="index.html">Home</a>

## Introduction

The purpose of this section is to review and revise the previous sections to address peer and instructor review as well as to discuss future directions of this project, including the implications of the insights and how to operationalize the work.

## Revisions: Part 1

1. Provided additional numerical proof alongside the graph that indicates that the higher seeded players wins roughly 2/3 of the time. The following line of code was added for this purpose:

```{r eval=FALSE}
sum(match$winner_seed<match$loser_seed)/nrow(match)
```

2. Changed the formatting of the variable list for easier readability.

3. Included links to every other page for easier navigation.

4. Made graphs more aesthetically pleasing with added color and adjusted legend titles.

## Revisions: Part 2

1. Added tourney_slug to final table to make adding court surface data easier.

## Extra Data

A fellow student gave me some feedback regarding the model from Part 2, saying that it would be interesting to see if the court surface might be a good predictor. There might be datasets out there that include the court surface for each tournament, but I compiled the following lists myself after 30 minutes of Googling.

```{r}
hard_tournaments = c("brisbane", "chennai", "doha", "sydney", "auckland", "australian-open", "montpellier", "sofia", "memphis", "rotterdam", "delray-beach", "marseille", "acapulco", "dubai", "indian-wells", "miami", "atlanta", "los-cabos", "washington", "montreal", "cincinnati", "winston-salem", "us-open", "metz", "st-petersburg", "chengdu", "shenzhen", "beijing", "tokyo", "shanghai", "antwerp", "moscow", "stockholm", "basel", "vienna", "paris", "nitto-atp-finals")
grass_tournaments = c("s-hertogenbosch", "stuttgart", "halle", "london", "antalya", "eastbourne", "wimbledon", "newport")
clay_tournaments = c("quito", "buenos-aires", "rio-de-janeiro", "sao-paulo", "houston", "marrakech", "monte-carlo", "barcelona", "budapest", "estoril", "istanbul", "munich", "madrid", "rome", "geneva", "lyon", "roland-garros", "bastad", "umag", "gstaad", "hamburg", "kitzbuhel")
```

Now let's load our data from Part 2.

```{r message=FALSE, error=FALSE, warning=FALSE, results='hide'}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("tidyverse")
include("knitr")
purl("deliverable2.Rmd", output = "part2.r")
source("part2.r")
```

## Cross-Validation

We used cross-validation on our model in Part 2, but here are the relevant values.

**R2**: 0.915  
Roughly 91.5% of the variability in our test data can be explained by our model, which is quite high.

**RMSE**: 7.449  
Our model's predictions for winner_points_won were off by an average of 7.5

**RMSE/mean**: 0.089  
Our prediction error rate is very close to 0, which indicates that the model is quite accurate at predicting the test data.

## Operationalization (or lack thereof)

The insights from this project seem to indicate that operationalization is either not necessary or not really possible.

Part 1 showed that while the current seeding system is not perfect, it is certainly good enough, so there is no real need to pursue any sort of change to the system.

The predictions from Part 2 have to do with post-game stats and while some of the conclusions are interesting, they don't indicate a necessity or avenue for change.

<a href="deliverable1.html">Part 1</a>

<a href="deliverable2.html">Part 2</a>
