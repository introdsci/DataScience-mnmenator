---
title: "Results and Operationalization"
author: "Michael Messmer"
output:
  html_document:
    df_print: paged
---
<a href="index.html">Home</a>

## Introduction

The purpose of this section is to review and revise the previous sections to address peer and instructor review as well as to discuss future directions of this project, including the implications of the insights and how to operationalize the work.

## Revisions: Part 1

1. Provided additional numerical proof alongside the graph that indicates that the higher seeded players wins roughly 2/3 of the time. The following line of code was added for this purpose:

```{r eval=FALSE}
sum(match$winner_seed<match$loser_seed)/nrow(match)
```

2. Changed the formatting of the variable list for easier readability.

3. Included links to every other page for easier navigation.

4. Made graphs more aesthetically pleasing with added color and adjusted legend titles.

## Revisions: Part 2

1. Added tourney_slug to final table to make adding court surface data easier.

2. Changed the formatting of the variable list for easier readability.

3. Included links to every other page for easier navigation.

## Cross-Validation

We used cross-validation on our model in Part 2, but here are the relevant values.

**R2**: 0.915  
Roughly 91.5% of the variability in our test data can be explained by our model, which is quite high.

**RMSE**: 7.449  
Our model's predictions for winner_points_won were off by an average of 7.5

**RMSE/mean**: 0.089  
Our prediction error rate is very close to 0, which indicates that the model is quite accurate at predicting the test data.

## Extra Data

A fellow student gave me some feedback regarding the model from Part 2, saying that it would be interesting to see if the court surface might be a good predictor. There might be datasets out there that include the court surface for each tournament, but I compiled the following lists myself after 30 minutes of Googling.

```{r}
hard_tournaments = c("brisbane", "chennai", "doha", "sydney", "auckland", "australian-open", "montpellier", "sofia", "memphis", "rotterdam", "delray-beach", "marseille", "acapulco", "dubai", "indian-wells", "miami", "atlanta", "los-cabos", "washington", "montreal", "cincinnati", "winston-salem", "us-open", "metz", "st-petersburg", "chengdu", "shenzhen", "beijing", "tokyo", "shanghai", "antwerp", "moscow", "stockholm", "basel", "vienna", "paris", "nitto-atp-finals")
grass_tournaments = c("s-hertogenbosch", "stuttgart", "halle", "london", "antalya", "eastbourne", "wimbledon", "newport")
clay_tournaments = c("quito", "buenos-aires", "rio-de-janeiro", "sao-paulo", "houston", "marrakech", "monte-carlo", "barcelona", "budapest", "estoril", "istanbul", "munich", "madrid", "rome", "geneva", "lyon", "roland-garros", "bastad", "umag", "gstaad", "hamburg", "kitzbuhel")
```

Now let's load our data from Part 2.

```{r message=FALSE, error=FALSE, warning=FALSE, results='hide'}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("tidyverse")
include("knitr")
purl("deliverable2.Rmd", output = "part2.r")
source("part2.r")
```

Let's add the court surfaces to our table.

```{r}
ratios$surface <- ""

for(tournament in hard_tournaments){
  ratios$surface[ratios$tourney_slug == tournament] <- "hard"
}

for(tournament in clay_tournaments){
  ratios$surface[ratios$tourney_slug == tournament] <- "clay"
}

for(tournament in grass_tournaments){
  ratios$surface[ratios$tourney_slug == tournament] <- "grass"
}
```

Right now the surface data is encoded as a string by default, let's factorize it to make it more fit for the model.

```{r}
ratios$surface <- as.factor(ratios$surface)
head(ratios)
```

Now that we've properly added the surface data to our table, let's go through the model building process again and see if surface is a good predictor.

```{r}
train <- ratios[sample_selection, ]
test <- ratios[-sample_selection, ]
train_model <- lm(winner_total_points_won ~ winner_seed + winner_service_point_ratio + winner_break_point_ratio + winner_return_point_ratio + loser_seed + loser_break_point_ratio + match_duration + surface, data=train)
summary(train_model)
```

The p-values for surface are less than .05, so it looks like the surface is a statistically significant predictor of winner_total_points_won. Let's see if we can make sense of the values.

Because surface is a categorical variable, the estimate values are relative to the default level, which in this case is clay. This means that when compared to clay courts, the winner has to win more points to win the overall match on hard and grass courts, over doubly so for grass. This seems to indicate that matches played on grass courts tend to be closer than hard, with both being closer than clay.

A possible explanation for grass having the closest matches is that grass courts favor big serves much more than other courts. This means that players tend to win their service games even more decisively than normal, leading to long matches that usually go to tiebreaks.

The majority of tournaments are played on hard court, so most players are most familiar with that surface. Hard courts are essentially the middle ground between grass and clay in terms of court conditions, so it makes sense that it is the middle ground for our model.

Clay is a very polarizing surface. In contrast to hard, many players either excel at playing on clay or play much worse on clay than on the other surfaces. This would lead to more blowout matches where the winner didn't need as many points.

## Testing the New Model

```{r}
predictions <- train_model %>% predict(test)
R2(predictions, test$winner_total_points_won)
```

Our new model explains slightly more of the variability in our test data than our old model, going from 91.5% to 92.3%, which means that adding the surface data was an improvement.

```{r}
RMSE(predictions, test$winner_total_points_won)
```

Our RMSE value has improved as well. The model's average error is down to 7.1 from 7.4, meaning that our model's predictions are more accurate.

```{r}
RMSE(predictions, test$winner_total_points_won)/mean(test$winner_total_points_won)
```

Our prediction error rate is also down to 0.084 from 0.089. All of our metrics support that the surface type is a statistically significant predictor of winner_total_points_won.

## Operationalization (or lack thereof)

The insights from this project seem to indicate that operationalization is either not necessary or not really possible.

Part 1 showed that while the current seeding system is not perfect, it is certainly good enough, so there is no real need to pursue any sort of change to the system.

The predictions from our model have to do with post-game stats, which means that predictions can't really be made in advance. While some of the conclusions are interesting they don't reveal anything that isn't already well known, such as break point conversions being important, so I wouldn't say they indicate a necessity or avenue for change. 

<a href="deliverable1.html">Part 1</a>

<a href="deliverable2.html">Part 2</a>
